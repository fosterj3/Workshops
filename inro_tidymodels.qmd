---
title: "Introduction to Tidymodels"
format: html
editor: visual
---

# Workshop Notes

**Types of Machine Learning**

-tidymodels: supervised machine learning

-tidyclust: unsupervised machine learning

**Predictor criteria/checklist**

-ethical

-availability

-contribute to explainability

75/25 split is the default split

**Splitting Data**

When should you split your data? Answer: As soon as possible. Attending to missingness should happen after the split

Guidance for splitting the data: Should focus on testing data to determine how much data you need to get reliable performance (typically more than 1,000 rows is pretty good)

**Ways to fit a Linear Model**

-lm: for linear model

-glmnet: for regularized regression

-keras: for regression using TensorFlow

-stan: for Bayesian regression

-spark: for large data sets

-brulee: for regression using torch

*To specify a model*

1)  Choose a model

2)  Specify a model (e.g. lm, glmnet, keras, etc.) 3) Set the mode

*find a model*

https://www.tidymodels.org/find/parsnip/

*help page for specific models*

https://parsnip.tidymodels.org//reference/index.html

**Modelling Guidelines**

Begin with a workflow!

-Workflows handle new data better than base R tools in terms of new factor levels.

-You can use other preprocessors besides formulas (more on feature engineering in Advanced tidymodels).

-They can help organize your work when working with multiple models.

-A workflow captures the entire modeling process

**Resampling Methods**

*Cross-Validation* randomly splits the data into V groups of roughly equal size (called "folds").

*Monte-Carlo Cross Validation*

*Bootstrapping* is a resampling procedure that uses data from one sample to generate a sampling distribution by repeatedly taking random samples from the known sample, with replacement.

**Types of ML Models**

A random forest empirically performs better than a single decision tree

**Tuning Models**

Some model or preprocessing parameters cannot be estimated directly from the data.

Some examples: -Tree depth in decision trees

-Number of neighbors in a K-nearest neighbor model

**References**

-Course Website: https://workshops.tidymodels.org/

-GitHub: https://github.com/tidymodels/workshops/blob/main/classwork/intro-04-classwork.qmd

-TidyModels Website: https://www.tidymodels.org/

-TidyModeling with R Book: https://www.tmwr.org/

-Applied Machine Learning Book: https://aml4td.org/chapters/introduction.html

------------------------------------------------------------------------

# Install Packages

```{r}
# Install the packages for the workshop
pkgs <- 
  c("bonsai", "Cubist", "doParallel", "earth", "embed", "finetune", 
    "forested", "lightgbm", "lme4", "parallelly", "plumber", "probably", 
    "ranger", "rpart", "rpart.plot", "rules", "splines2", "stacks", 
    "text2vec", "textrecipes", "tidymodels", "vetiver")

install.packages(pkgs)
```

# Load in Packages

```{r}
library(tidymodels)
library(forested)
```

# Examine the Data

```{r}
View(forested)
```

# Split the Data

```{r}
set.seed(123)
forested_split <- initial_split(forested)
forested_split
```

```{r}
forested_train <- training(forested_split)
forested_test <- testing(forested_split) # Don't examine
```

```{r}
# Split the data into 80/20
set.seed(123)
forested_second_split <- initial_split(forested, prop = .80)
```

```{r}
# Access second split data 
forested_train_two <- training(forested_second_split)
forested_test_two <- testing(forested_second_split) # Don't examine
```

# Exploratory Data Analysis

```{r}
summary(forested_train_two)
```

```{r}
forested_train_two %>% 
  ggplot(aes(x=forested)) +
  geom_bar()
```

```{r}
# Visualize the data 
forested_train_two %>% 
ggplot(aes(elevation)) +
  geom_histogram() 
```

```{r}
forested_train_two %>% 
  ggplot(aes(x=forested)) +
  geom_bar(aes(fill = land_type), position = "dodge")
```

```{r}
forested_train_two %>% 
  ggplot(aes(x = lon, y = lat, col = forested)) +
  geom_point()
```

# Understanding models

```{r}
tree_spec <- logistic_reg() %>% 
  set_engine("stan")
tree_spec
```

# Introduction to Modeling

```{r}
# run a workflow to fit the model
tree_spec <-
  decision_tree() %>% 
  set_mode("classification")

tree_fit <-
  workflow(forested ~ ., tree_spec) %>% 
  fit(data = forested_train_two)  
```

```{r}
# Use predict function
predict(tree_fit, new_data = forested_test_two)
```

```{r}
augment(tree_fit, new_data = forested_test_two) %>% 
  View()
```

# Understanding your model

```{r}
tree_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot::rpart.plot(roundint = FALSE)
```

# Extracting model engine object from your fitted workflow

```{r}

```

# Evaluating the Model

```{r}
# confusion matrix
augment(tree_fit, new_data = forested_train_two) %>% 
  conf_mat(truth = forested, estimate = .pred_class)
```

```{r}
# accuracy checks the how well the model predicts overall
augment(tree_fit, new_data = forested_train_two) %>% 
  accuracy(truth = forested, estimate = .pred_class)

#Sensitivity checks how well the model predicts the true "yes"
augment(tree_fit, new_data = forested_train_two) %>% 
  sensitivity(truth = forested, estimate = .pred_class)

# Specificity checks how well the model predicts the no's
augment(tree_fit, new_data = forested_train_two) %>% 
  specificity(truth = forested, estimate = .pred_class)
```

```{r}
# Examine accuracy, specificity, and sensitivity at the same time
forested_metrics <- metric_set(accuracy, specificity, sensitivity)

augment(tree_fit, new_data = forested_train_two) %>% 
  forested_metrics(truth = forested, estimate = .pred_class)
```

```{r}
#How well does your model perform for different groups?
augment(tree_fit, new_data = forested_train_two) %>%
  group_by(tree_no_tree) %>%
  forested_metrics(truth = forested, estimate = .pred_class)
```

```{r}
#Examine the ROC curve
augment(tree_fit, new_data = forested_train_two) %>%
  roc_curve(truth = forested, .pred_Yes)
```

```{r}
#Examine the area under the curve (AUC)
augment(tree_fit, new_data = forested_train_two) %>%
  roc_auc(truth = forested, .pred_Yes)
```

```{r}
# Examine the Brier score 
  ## the brier score is analogous to the mean squared error in regression models
  ## it captures calibration, the ROC captures separation 
  ## smaller values are better, for binary classification the "bad model threshold" is about 0.25
augment(tree_fit, new_data = forested_train_two) %>%
  brier_class(truth = forested, .pred_Yes)
```

# Overfitting

```{r}
# You should not do this. This is merely for educational purposes 
tree_fit %>% 
  augment(forested_train_two) %>% 
  brier_class(forested, .pred_Yes)

tree_fit %>% 
  augment(forested_test_two) %>% 
  brier_class(forested, .pred_Yes)
```

# How to evaluate different models

```{r}
# default validation is 10-fold
set.seed(123)
forested_folds <- vfold_cv(forested_train_two)

forested_folds$splits[1:3]
```

```{r}
# Changing the number of folds
set.seed(321) 
vfold_cv(forested_train_two, v = 5)
```

```{r}
# Monte-Carlo Cross Validation set
set.seed(123)
mc_cv(forested_train_two, prop = 9/10, times = 10)

```

# Create a random forest

```{r}
rf_spec <- rand_forest(trees = 1000, mode = "classification")
rf_spec
```

#Workflow

```{r}
rf_wflow <- workflow(forested ~ ., rf_spec)
rf_wflow
```

```{r}
ctrl_forested <- control_resamples(save_pred = TRUE)

# Random forest uses random numbers so set the seed first

set.seed(2)
rf_res <- fit_resamples(rf_wflow, forested_folds, control = ctrl_forested)
collect_metrics(rf_res)
```

```{r}
# forested_split has train + test info
final_fit <- last_fit(rf_wflow, forested_split) 

final_fit
```
